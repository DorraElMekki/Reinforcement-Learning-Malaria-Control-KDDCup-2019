{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-e0jnPxVR3ZL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/slremy/netsapi\n",
      "  Cloning https://github.com/slremy/netsapi to c:\\users\\karim\\appdata\\local\\temp\\pip-req-build-2b9943nq\n",
      "Building wheels for collected packages: netsapi\n",
      "  Building wheel for netsapi (setup.py): started\n",
      "  Building wheel for netsapi (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\Karim\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-nju1to5r\\wheels\\9e\\73\\c9\\86a9cc2460e11b3ce5b0a5ebd2d9d332a68afe0941659967fa\n",
      "Successfully built netsapi\n",
      "Installing collected packages: netsapi\n",
      "  Found existing installation: netsapi 1.2\n",
      "    Uninstalling netsapi-1.2:\n",
      "      Successfully uninstalled netsapi-1.2\n",
      "Successfully installed netsapi-1.2\n",
      "105  Evaluations Remaining\n",
      "100  Evaluations Remaining\n",
      "95  Evaluations Remaining\n",
      "90  Evaluations Remaining\n",
      "85  Evaluations Remaining\n",
      "normalize  39.60080686223995\n",
      "80  Evaluations Remaining\n",
      "75  Evaluations Remaining\n",
      "70  Evaluations Remaining\n",
      "65  Evaluations Remaining\n",
      "60  Evaluations Remaining\n",
      "55  Evaluations Remaining\n",
      "50  Evaluations Remaining\n",
      "45  Evaluations Remaining\n",
      "40  Evaluations Remaining\n",
      "35  Evaluations Remaining\n",
      "30  Evaluations Remaining\n",
      "25  Evaluations Remaining\n",
      "20  Evaluations Remaining\n",
      "15  Evaluations Remaining\n",
      "10  Evaluations Remaining\n",
      "5  Evaluations Remaining\n",
      "{1: [0.5680179599161836, 0.13791049539488898], 2: [0.9367910864350734, 0.2747311773414316], 3: [0.7770450390438369, 0.21383822641963457], 4: [0.9178566071427676, 0.04140855941079136], 5: [0.690260444228248, 0.882836823828347]} 157.28896658679162\n",
      "105  Evaluations Remaining\n",
      "100  Evaluations Remaining\n",
      "95  Evaluations Remaining\n",
      "90  Evaluations Remaining\n",
      "85  Evaluations Remaining\n",
      "normalize  88.4524840702977\n",
      "80  Evaluations Remaining\n",
      "75  Evaluations Remaining\n",
      "70  Evaluations Remaining\n",
      "65  Evaluations Remaining\n",
      "60  Evaluations Remaining\n",
      "55  Evaluations Remaining\n",
      "50  Evaluations Remaining\n",
      "45  Evaluations Remaining\n",
      "40  Evaluations Remaining\n",
      "35  Evaluations Remaining\n",
      "30  Evaluations Remaining\n",
      "25  Evaluations Remaining\n",
      "20  Evaluations Remaining\n",
      "15  Evaluations Remaining\n",
      "10  Evaluations Remaining\n",
      "5  Evaluations Remaining\n",
      "{1: [1.0, 0.0], 2: [0.0, 1.0], 3: [1.0, 0.0], 4: [1.0, 1.0], 5: [0.08889876760071834, 0.3532098556828534]} 402.2603603844685\n",
      "105  Evaluations Remaining\n",
      "100  Evaluations Remaining\n",
      "95  Evaluations Remaining\n",
      "90  Evaluations Remaining\n",
      "85  Evaluations Remaining\n",
      "normalize  56.91140877161871\n",
      "80  Evaluations Remaining\n",
      "75  Evaluations Remaining\n",
      "70  Evaluations Remaining\n",
      "65  Evaluations Remaining\n",
      "60  Evaluations Remaining\n",
      "55  Evaluations Remaining\n",
      "50  Evaluations Remaining\n",
      "45  Evaluations Remaining\n",
      "40  Evaluations Remaining\n",
      "35  Evaluations Remaining\n",
      "30  Evaluations Remaining\n",
      "25  Evaluations Remaining\n",
      "20  Evaluations Remaining\n",
      "15  Evaluations Remaining\n",
      "10  Evaluations Remaining\n",
      "5  Evaluations Remaining\n",
      "{1: [0.6989045120592535, 0.0907487154726293], 2: [0.011052063807820867, 0.20500636374239567], 3: [0.14296335709972496, 0.998611832653475], 4: [0.6375239757767118, 0.6791514084578412], 5: [0.3022942042825406, 0.04550890660161422]} 171.9354436772328\n",
      "105  Evaluations Remaining\n",
      "100  Evaluations Remaining\n",
      "95  Evaluations Remaining\n",
      "90  Evaluations Remaining\n",
      "85  Evaluations Remaining\n",
      "normalize  29.571512325242935\n",
      "80  Evaluations Remaining\n",
      "75  Evaluations Remaining\n",
      "70  Evaluations Remaining\n",
      "65  Evaluations Remaining\n",
      "60  Evaluations Remaining\n",
      "55  Evaluations Remaining\n",
      "50  Evaluations Remaining\n",
      "45  Evaluations Remaining\n",
      "40  Evaluations Remaining\n",
      "35  Evaluations Remaining\n",
      "30  Evaluations Remaining\n",
      "25  Evaluations Remaining\n",
      "20  Evaluations Remaining\n",
      "15  Evaluations Remaining\n",
      "10  Evaluations Remaining\n",
      "5  Evaluations Remaining\n",
      "{1: [1.0, 0.0], 2: [1.0, 0.0], 3: [0.0, 1.0], 4: [1.0, 0.612557016909855], 5: [0.8555245156326144, 0.3201994839566298]} 344.23051260818454\n",
      "105  Evaluations Remaining\n",
      "100  Evaluations Remaining\n",
      "95  Evaluations Remaining\n",
      "90  Evaluations Remaining\n",
      "85  Evaluations Remaining\n",
      "normalize  50.201558945272126\n",
      "80  Evaluations Remaining\n",
      "75  Evaluations Remaining\n",
      "70  Evaluations Remaining\n",
      "65  Evaluations Remaining\n",
      "60  Evaluations Remaining\n",
      "55  Evaluations Remaining\n",
      "50  Evaluations Remaining\n",
      "45  Evaluations Remaining\n",
      "40  Evaluations Remaining\n",
      "35  Evaluations Remaining\n",
      "30  Evaluations Remaining\n",
      "25  Evaluations Remaining\n",
      "20  Evaluations Remaining\n",
      "15  Evaluations Remaining\n",
      "10  Evaluations Remaining\n",
      "5  Evaluations Remaining\n",
      "{1: [0.4908045054318865, 0.08937373887703004], 2: [0.1622813973551318, 0.6187805640255127], 3: [0.7474396874617407, 0.5186513330547943], 4: [0.3980208215414369, 0.3138758648331572], 5: [0.0718020571563237, 0.22393412717499628]} 169.89175351885632\n",
      "105  Evaluations Remaining\n",
      "100  Evaluations Remaining\n",
      "95  Evaluations Remaining\n",
      "90  Evaluations Remaining\n",
      "85  Evaluations Remaining\n",
      "normalize  55.68198728060912\n",
      "80  Evaluations Remaining\n",
      "75  Evaluations Remaining\n",
      "70  Evaluations Remaining\n",
      "65  Evaluations Remaining\n",
      "60  Evaluations Remaining\n",
      "55  Evaluations Remaining\n",
      "50  Evaluations Remaining\n",
      "45  Evaluations Remaining\n",
      "40  Evaluations Remaining\n",
      "35  Evaluations Remaining\n",
      "30  Evaluations Remaining\n",
      "25  Evaluations Remaining\n",
      "20  Evaluations Remaining\n",
      "15  Evaluations Remaining\n",
      "10  Evaluations Remaining\n",
      "5  Evaluations Remaining\n",
      "{1: [0.8651372310862415, 0.0], 2: [0.9018681041676481, 0.0], 3: [0.0, 1.0], 4: [1.0, 0.7238932513843523], 5: [1.0, 0.3155509541076346]} 275.4905797038468\n",
      "105  Evaluations Remaining\n",
      "100  Evaluations Remaining\n",
      "95  Evaluations Remaining\n",
      "90  Evaluations Remaining\n",
      "85  Evaluations Remaining\n",
      "normalize  84.69833340424785\n",
      "80  Evaluations Remaining\n",
      "75  Evaluations Remaining\n",
      "70  Evaluations Remaining\n",
      "65  Evaluations Remaining\n",
      "60  Evaluations Remaining\n",
      "55  Evaluations Remaining\n",
      "50  Evaluations Remaining\n",
      "45  Evaluations Remaining\n",
      "40  Evaluations Remaining\n",
      "35  Evaluations Remaining\n",
      "30  Evaluations Remaining\n",
      "25  Evaluations Remaining\n",
      "20  Evaluations Remaining\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "#!pip3 install git+https://github.com/slremy/netsapi --user --upgrade\n",
    "\n",
    "from netsapi.challenge import *\n",
    "class CustomAgent():\n",
    "    \n",
    "    def __init__(self, environment):\n",
    "        \n",
    "        #Hyperparameters\n",
    "        self.env = environment\n",
    "        self.epsilon = 0.9\n",
    "        self.gamma = 0.9\n",
    "        self.action_resolution = 0.1\n",
    "        self.action_resolution_year1 = 0.3\n",
    "        self.Q = defaultdict(lambda : 0.) # Q-function\n",
    "        self.n = defaultdict(lambda : 1.) # number of visits\n",
    "        self.actions = self.actionSpace(self.action_resolution)\n",
    "        self.actionspace = range(len(self.actions)-1)\n",
    "        self.policymax = []\n",
    "        self.actionyear1 = self.actionSpace(self.action_resolution_year1)\n",
    "        self.actionspaceyear1 = range(len(self.actionyear1)-1)\n",
    "        self.memory = []\n",
    "        \n",
    "    \n",
    "    def actionSpace(self,resolution):\n",
    "        x,y = np.meshgrid(np.arange(0,1.1,resolution), np.arange(0,1.1,resolution))\n",
    "        xy = np.concatenate((x.reshape(-1,1), y.reshape(-1,1)), axis=1)\n",
    "        return xy.round(2).tolist()\n",
    "    def exploitSpace(self,action,resolution):\n",
    "        cactionspace = []\n",
    "        final = []\n",
    "        for i in [resolution,0,-resolution]:\n",
    "            for j in [resolution,0,-resolution]:\n",
    "                cactionspace.append([action[0]+j,action[1]+i])\n",
    "        for a in cactionspace:\n",
    "            if(a not in self.memory and a[0]<=1 and a[0]>=0 and a[1]<=1 and a[1]>=0):\n",
    "                final.append(a)\n",
    "#         print(\"final: \", final)\n",
    "        return final\n",
    "    def train(self):\n",
    "        rewardmax = -999999\n",
    "        policymax = []\n",
    "        currentReward = 0\n",
    "        Q = self.Q\n",
    "        n = self.n\n",
    "        gamma = self.gamma\n",
    "        actions = self.actions\n",
    "        actionspace = self.actionspace\n",
    "        actionyear1 = self.actionyear1\n",
    "        actionspaceyear1 = self.actionyear1\n",
    "        currentPolicy = []\n",
    "        maxactionyear1 = []\n",
    "        greedy_action = lambda s : max(actionspace, key=lambda a : Q[(s,a)])\n",
    "        max_q = lambda sp : max([Q[(sp,a)] for a in actionspace])\n",
    "        rewardmaxyear1 = -9999\n",
    "        count = 20 # 20 evaluations = 4 policies\n",
    "        #find action for the first year with 20 evaluations\n",
    "        for a in actionyear1:\n",
    "            \n",
    "            tempa = a\n",
    "            count-=1\n",
    "            self.env.reset()\n",
    "            _,reward,_,_ = self.env.evaluateAction(tempa);\n",
    "#             print(\"57: \", reward, \" \", tempa)\n",
    "            self.memory.append(tempa)\n",
    "            if(reward > rewardmaxyear1):\n",
    "                rewardmaxyear1 = reward\n",
    "                maxactionyear1 = tempa\n",
    "                \n",
    "        spaceExploit = self.exploitSpace(maxactionyear1,self.action_resolution)\n",
    "        while(count>0):\n",
    "            self.env.reset()\n",
    "            nextaction = []\n",
    "            direct = 0\n",
    "            if(direct == 1):\n",
    "                actionchoice = nextaction\n",
    "            else:\n",
    "                actionchoice = random.choice(spaceExploit)\n",
    "            if(actionchoice not in self.memory):\n",
    "                self.env.reset()\n",
    "                _,reward,_,_ = self.env.evaluateAction(actionchoice)\n",
    "#                 print(\"74: \",reward, \" \", actionchoice)\n",
    "                count-=1\n",
    "                self.memory.append(actionchoice)\n",
    "                direction = [actionchoice[0] - maxactionyear1[0],actionchoice[1] - maxactionyear1[1]]\n",
    "                if(reward > rewardmaxyear1):\n",
    "                    rewardmaxyear1 = reward\n",
    "                    maxactionyear1 = actionchoice\n",
    "                    nextaction = [actionchoice[0] + direction[0],actionchoice[1] + direction[1]]\n",
    "                    direct =1\n",
    "                    if(nextaction[0] >1 or nextaction[0] <0 or nextaction[1] >1 or nextaction[1] <0):\n",
    "                        nextaction = [actionchoice[0] - direction[0],actionchoice[1] - direction[1]]\n",
    "                        spaceExploit = self.exploitSpace(nextaction,self.action_resolution)\n",
    "                        direct = 0\n",
    "#                 if(spaceExploit.index[actionchoice])\n",
    "#                 spaceExploit.remove(actionchoice)\n",
    "        for e in range(16-5): #16 policies left\n",
    "            epsilon = 0.8-(e/(16*1.2))\n",
    "            self.env.reset()\n",
    "            nextstate = self.env.state\n",
    "            currentReward = 0\n",
    "            currentPolicy=[]\n",
    "#             print(maxactionyear1,\" \", rewardmaxyear1)\n",
    "            while True:\n",
    "                state = nextstate\n",
    "\n",
    "                # Epsilon-Greedy Action Selection\n",
    "                if epsilon > random.random() :\n",
    "                    action = random.choice(actionspace)\n",
    "                else :\n",
    "                    action = greedy_action(state)\n",
    "                n[(state,action)]+=1\n",
    "                env_action = actions[action]#convert to ITN/IRS\n",
    "                #print('env_action', env_action)\n",
    "                if(state == 1 ):\n",
    "                    env_action = maxactionyear1\n",
    "                nextstate, reward, done, _ = self.env.evaluateAction(env_action)\n",
    "                currentReward += reward\n",
    "                currentPolicy.append(env_action)\n",
    "                # Q-learning\n",
    "                if done :\n",
    "                    Q[(state,action)] = Q[(state,action)] + 1./n[(state,action)] * ( reward - Q[(state,action)] )\n",
    "                    if(currentReward > rewardmax):\n",
    "#                         print(rewardmax)\n",
    "                        rewardmax = currentReward\n",
    "                        self.policymax = currentPolicy[:]\n",
    "#                         print(self.policymax)\n",
    "                    break\n",
    "                else :\n",
    "                    Q[(state,action)] = Q[(state,action)] + 1./n[(state,action)] * ( reward + gamma * max_q(nextstate) - Q[(state,action)] )\n",
    "            \n",
    "            if (e==8):# kim get in\n",
    "                bo = BayesianOptimization(f=self.target,\n",
    "                                       pbounds={'Y1x': (self.x_start, self.x_end), 'Y1y': (self.x_start, self.x_end), 'Y2x': (self.x_start, self.x_end), 'Y2y': (self.x_start, self.x_end), 'Y3x': (self.x_start, self.x_end), 'Y3y': (self.x_start, self.x_end), 'Y4x': (self.x_start, self.x_end), 'Y4y': (self.x_start, self.x_end), 'Y5x': (self.x_start, self.x_end), 'Y5y': (self.x_start, self.x_end)},\n",
    "                                       verbose=0)\n",
    "                for p,r in zip(next_point_to_probes,firstRewards):\n",
    "                bo.register(\n",
    "                    params=p,\n",
    "                    target= r / self.normalize,\n",
    "                    )\n",
    "            \n",
    "            next_point_to_probe = {'Y1x': Y1x, 'Y1y': Y1y, 'Y2x': Y2x, 'Y2y': Y2y, 'Y3x':Y3x, 'Y3y': Y3y, 'Y4x': Y4x, 'Y4y': Y4y, 'Y5x':Y5x, 'Y5y':Y5y}\n",
    "            \n",
    "        return Q\n",
    "\n",
    "\n",
    "    def generate(self):\n",
    "        best_policy = None\n",
    "        best_reward = -float('Inf')\n",
    "        Q_trained = self.train()\n",
    "#         greedy_eval = lambda s : max(self.actionspace, key=lambda a : Q_trained[(s,a)])\n",
    "#         print(self.policymax)\n",
    "        best_policy = {state : (self.policymax[state-1]) for state in range(1,6)}\n",
    "        best_reward = self.env.evaluatePolicy(best_policy)\n",
    "        \n",
    "        print(best_policy, best_reward)\n",
    "        \n",
    "        return best_policy, best_reward"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BayesianCup_2D_2years_fixedTL.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
