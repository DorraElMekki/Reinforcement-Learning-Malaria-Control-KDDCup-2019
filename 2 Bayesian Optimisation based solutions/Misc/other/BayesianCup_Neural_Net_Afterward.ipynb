{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-e0jnPxVR3ZL"
   },
   "outputs": [],
   "source": [
    "#!pip install bayesian-optimization  # for google collab\n",
    "#!pip install git+https://github.com/slremy/netsapi --user --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 779,
     "status": "ok",
     "timestamp": 1561628162197,
     "user": {
      "displayName": "sephiros sama",
      "photoUrl": "https://lh5.googleusercontent.com/-Aaghu78j1FA/AAAAAAAAAAI/AAAAAAAAImI/of29pyh0eh4/s64/photo.jpg",
      "userId": "04364851670955414673"
     },
     "user_tz": -120
    },
    "id": "crzCGgxjNGqw",
    "outputId": "5296003c-401f-48c5-f568-d1c0cec2e36f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt.util import UtilityFunction\n",
    "from netsapi.challenge import *\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib import mlab\n",
    "from matplotlib import gridspec\n",
    "%matplotlib inline\n",
    "from sys import exit, exc_info, argv\n",
    "from multiprocessing import Pool, current_process\n",
    "import random as rand\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from contextlib import contextmanager\n",
    "import sys, os\n",
    "@contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:  \n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import math\n",
    "\n",
    "def newHoleEnv():\n",
    "    return ChallengeProveEnvironment()\n",
    "    #return ChallengeSeqDecEnvironment() #Initialise a New Challenge Environment to post entire policy\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility : Neural net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# action val in\n",
    "x_start = 0.0\n",
    "x_end = 1.0\n",
    "\n",
    "old_action = np.array([0.0 , 0.0])\n",
    "weight = np.array([0.0 , 0.0]) # todo init rand\n",
    "\n",
    "\n",
    "def nn_sum(w0, w1, x1): # affine\n",
    "    return w0 + w1*x1\n",
    "def nn_mul(a,b):\n",
    "    return a*b\n",
    "def nn_Modified_sigmoid(x):\n",
    "    return nn_sigmoid(x)/2 + 0.5\n",
    "def nn_sigmoid(x):\n",
    "    return (math.exp( 2.0*x ) - 1.0 ) / (math.exp( 2.0*x ) + 1.0)\n",
    "def nn_porte(x, mn, mx):\n",
    "    return max(mn , min(x,mx) )\n",
    "\n",
    "def nn(action):\n",
    "    global old_action\n",
    "    a1 = nn_sum(weight[0], weight[1], old_action)\n",
    "    a2 = nn_sum(weight[0], weight[1], action)\n",
    "    a3 = nn_mul(a1,a2)\n",
    "    a3 = nn_Modified_sigmoid(a3)\n",
    "    a4 = nn_porte(a3, x_start, x_end)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def nn_balance(w0, a1, a2):\n",
    "    return w0*a1 + (1.0-w0)*a2\n",
    "def nn_simple_sum(a,b):\n",
    "    return a+b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "policyID_queue = [] # Example [0,1,2]\n",
    "target_out = [] # example [1]\n",
    "\n",
    "\n",
    "# init\n",
    "policies = []\n",
    "detailed_rewards = []\n",
    "UsedPolicy = 0\n",
    "\n",
    "def target1(**kwargs):\n",
    "    global policyID_queue, detailed_rewards\n",
    "    reward = 0.0\n",
    "    if len( policyID_queue ) > 0 :\n",
    "        policyID = policyID_queue.pop(0)\n",
    "        detailed_reward = detailed_rewards[   policyID   ]\n",
    "        \n",
    "        for key in target_out:\n",
    "            #print(\"detailed_reward: \" , detailed_reward[key])\n",
    "            reward += detailed_reward[key]\n",
    "    else: # do it online   \n",
    "        raise SyntaxError('mini BOs should never query online ')\n",
    "        #Raises:\n",
    "        #      Runtime error\n",
    "        #     mini BOs should never query online \n",
    "    #print(\"target1 \" , reward)    \n",
    "    return reward  #  /90.0  \n",
    "\n",
    "\n",
    "def target(**kwargs):  \n",
    "    modeLista = False\n",
    "    valueLen = 0\n",
    "    for key, value in kwargs.items(): \n",
    "        #print (\"target:: \",key,\" == \",value) \n",
    "        if type(value) is np.ndarray:\n",
    "            modeLista = True\n",
    "            valueLen = len(value)\n",
    "        break\n",
    "    if modeLista:\n",
    "        result = []\n",
    "        for _ in range(valueLen):\n",
    "            reward = target1()\n",
    "            result.append( reward )\n",
    "        return result\n",
    "    else:\n",
    "        return target1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_BO_hm(policy_trainSet):\n",
    "    global policyID_queue,target_out\n",
    "    \n",
    "    bos = [] \n",
    "    bos_target_out = []\n",
    "    \n",
    "    #TODO loop\n",
    "    bos.append( BayesianOptimization(target, {'1x': (x_start, x_end), '1y': (x_start, x_end)})   )\n",
    "    bos_target_out.append( [1] )\n",
    "    \n",
    "    for bo,aBO_target_out in zip(bos, bos_target_out):\n",
    "        policyID_queue = policy_trainSet[:]\n",
    "        target_out = aBO_target_out\n",
    "        #bo.maximize(init_points=5, n_iter=0, acq='ucb', kappa=10)\n",
    "        for apolicyID in policy_trainSet:\n",
    "            #print(\"queue: \", policyID_queue) it don t change here\n",
    "            #print(\"apolicyID \",apolicyID)\n",
    "            apolicy = policies[apolicyID]\n",
    "            #print(\"apolicy \",apolicy)\n",
    "            params = {}\n",
    "            for key in bo.space.keys:\n",
    "                params[key]=apolicy[key]\n",
    "            #print(\"probe for param: \", params )    \n",
    "            bo.probe(params=params,lazy=True)    \n",
    "            bo.maximize(init_points=0, n_iter=0, acq='ucb')\n",
    "        print(\"now queue should be empty: \", policyID_queue)\n",
    "    return bos\n",
    "\n",
    "\n",
    "def train():\n",
    "    #leave 1 out for the balance weights and MSE\n",
    "    for LeftPolicy in range(1): #todo put it back range(UsedPolicy):\n",
    "        ### set ids ...\n",
    "        print(\"UsedPolicy \", UsedPolicy)\n",
    "        policy_trainSet = list( range(UsedPolicy) )\n",
    "        policy_trainSet.pop(LeftPolicy)\n",
    "        print(\"policy_trainSet \", policy_trainSet)\n",
    "        \n",
    "        ## train BO hm\n",
    "        train_BO_hm(policy_trainSet)\n",
    "         \n",
    "        ## train weights\n",
    "        ## MSE\n",
    "    \n",
    "    # keep the best weights and BO hm regarding the MSE\n",
    "    # add the left policy to the hm\n",
    "    # 1 BO 10 dim\n",
    "    # 1 suggestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w02OmWsfNGq2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105  Evaluations Remaining\n",
      "104  Evaluations Remaining\n",
      "103  Evaluations Remaining\n",
      "102  Evaluations Remaining\n",
      "101  Evaluations Remaining\n",
      "100  Evaluations Remaining\n",
      "99  Evaluations Remaining\n",
      "98  Evaluations Remaining\n",
      "97  Evaluations Remaining\n",
      "96  Evaluations Remaining\n",
      "95  Evaluations Remaining\n",
      "94  Evaluations Remaining\n",
      "93  Evaluations Remaining\n",
      "92  Evaluations Remaining\n",
      "91  Evaluations Remaining\n",
      "90  Evaluations Remaining\n",
      "89  Evaluations Remaining\n",
      "88  Evaluations Remaining\n",
      "87  Evaluations Remaining\n",
      "86  Evaluations Remaining\n",
      "UsedPolicy  4\n",
      "policy_trainSet  [1, 2, 3]\n",
      "|   iter    |  target   |    1x     |    1y     |\n",
      "-------------------------------------------------\n",
      "target::  1x  ==  0.29547971537356377\n",
      "detailed_reward:  -0.32063986195114524\n",
      "target1  -0.32063986195114524\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.3206  \u001b[0m | \u001b[0m 0.2955  \u001b[0m | \u001b[0m 0.45    \u001b[0m |\n",
      "=================================================\n",
      "|   iter    |  target   |    1x     |    1y     |\n",
      "-------------------------------------------------\n",
      "target::  1x  ==  0.34256492512506664\n",
      "detailed_reward:  -0.3871706430146031\n",
      "target1  -0.3871706430146031\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-0.3872  \u001b[0m | \u001b[0m 0.3426  \u001b[0m | \u001b[0m 0.5846  \u001b[0m |\n",
      "=================================================\n",
      "|   iter    |  target   |    1x     |    1y     |\n",
      "-------------------------------------------------\n",
      "target::  1x  ==  0.08014829332019768\n",
      "detailed_reward:  -0.6411889971853739\n",
      "target1  -0.6411889971853739\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.6412  \u001b[0m | \u001b[0m 0.08015 \u001b[0m | \u001b[0m 0.6304  \u001b[0m |\n",
      "=================================================\n",
      "now queue should be empty:  []\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env = newHoleEnv()\n",
    "\n",
    "#hyper param\n",
    "x_start = 0.0\n",
    "x_end = 1.0\n",
    "normalisation = 90.0 # reward normalisation\n",
    "initial_rand_policy = 6 # 5 train + 1 test\n",
    "\n",
    "\n",
    "#code\n",
    "for _ in range(initial_rand_policy):\n",
    "        policy = {'1x':rand.random(),'1y':rand.random(),'2x':rand.random(),'2y':rand.random(),'3x':rand.random(),'3y':rand.random(),'4x':rand.random(),'4y':rand.random(),'5x':rand.random(),'5y':rand.random()}\n",
    "        detailed_reward= {1:0, 2:0, 3:0, 4:0, 5:0}\n",
    "        env.reset()\n",
    "        #reward_AllPolicy = env.evaluatePolicy(policy)\n",
    "        for i in range (1,6):\n",
    "            nextstate, reward, done, _ = env.evaluateAction([ policy[str(i)+'x'] , policy[str(i)+'y']  ] )\n",
    "            detailed_reward[i] = reward/ normalisation\n",
    "            \n",
    "        policies.append(policy)\n",
    "        detailed_rewards.append(detailed_reward)\n",
    "UsedPolicy = initial_rand_policy          \n",
    "\n",
    "#print(policies)\n",
    "#print(detailed_rewards)\n",
    "train()           \n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "BayesianCup_2D_2years_fixedTL.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
